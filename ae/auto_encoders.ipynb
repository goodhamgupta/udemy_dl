{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('../bm/ml-1m/movies.dat', sep=\"::\", header=None, engine='python', encoding='latin-1')\n",
    "users_df = pd.read_csv('../bm/ml-1m/users.dat', sep=\"::\", header=None, engine='python', encoding='latin-1')\n",
    "ratings_df = pd.read_csv('../bm/ml-1m/ratings.dat', sep=\"::\", header=None, engine='python', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Preparing the training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('../bm/ml-100k/u1.base', delimiter=\"\\t\", header=None, engine='python', encoding='latin-1')\n",
    "training_set = np.array(training_set, dtype = 'int')\n",
    "test_set = pd.read_csv('../bm/ml-100k/u1.test', delimiter = '\\t')\n",
    "test_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Get max number of movies and users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Convert training/test set into array with users as rows and movies as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        id_movies = data[:,1][data[:,0] == id_users]\n",
    "        id_ratings = data[:,2][data[:,0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Convert into torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "training_set = torch.Tensor(training_set)\n",
    "test_set = torch.Tensor(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Creating the neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- We are going to use Stacked AutoEncoders.\n",
    "- Create another class using inheritance to use the variables and the functions from the parent class module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Contains SAE architecture and helper functions\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SAE, self).__init__() # Trigger the base class init function\n",
    "        # Creating the full connections between input and hidden layer\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        # Creating the full connection between first and second hidden layer\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        # Creating the full connection for the decoding phase\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        # Final full connection\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        # Assign the activation function\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Function to perform forward propagation(encoding and decoding)\n",
    "        \"\"\"\n",
    "        output_fc1 = self.activation(self.fc1(x))\n",
    "        output_fc2 = self.activation(self.fc2(output_fc1))\n",
    "        output_fc3 = self.activation(self.fc3(output_fc2))\n",
    "        final_output = self.fc4(output_fc3)\n",
    "        return final_output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(params=model.parameters(), lr=0.01, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 1.108305105189679\n",
      "epoch: 2 loss: 1.0985055378219062\n",
      "epoch: 3 loss: 1.0965005213843748\n",
      "epoch: 4 loss: 1.094128705922906\n",
      "epoch: 5 loss: 1.0929039675519292\n",
      "epoch: 6 loss: 1.0906153169759198\n",
      "epoch: 7 loss: 1.0911172654434056\n",
      "epoch: 8 loss: 1.089521743202587\n",
      "epoch: 9 loss: 1.0897679931752646\n",
      "epoch: 10 loss: 1.088017195797968\n",
      "epoch: 11 loss: 1.0888474913036794\n",
      "epoch: 12 loss: 1.087759428104769\n",
      "epoch: 13 loss: 1.0875038561579053\n",
      "epoch: 14 loss: 1.0863453686743603\n",
      "epoch: 15 loss: 1.0875913779049342\n",
      "epoch: 16 loss: 1.0856944507807134\n",
      "epoch: 17 loss: 1.0862358167912962\n",
      "epoch: 18 loss: 1.086338665124423\n",
      "epoch: 19 loss: 1.0863118356508659\n",
      "epoch: 20 loss: 1.0848327463962848\n",
      "epoch: 21 loss: 1.0855504342310258\n",
      "epoch: 22 loss: 1.0840997557157437\n",
      "epoch: 23 loss: 1.0831156936514739\n",
      "epoch: 24 loss: 1.0789145517147634\n",
      "epoch: 25 loss: 1.07658559680136\n",
      "epoch: 26 loss: 1.0701003722942122\n",
      "epoch: 27 loss: 1.070201369274582\n",
      "epoch: 28 loss: 1.0597532588274357\n",
      "epoch: 29 loss: 1.0596189773388967\n",
      "epoch: 30 loss: 1.0525307058480868\n",
      "epoch: 31 loss: 1.0488120972035944\n",
      "epoch: 32 loss: 1.0435934975334122\n",
      "epoch: 33 loss: 1.0438567572484\n",
      "epoch: 34 loss: 1.0327332154746687\n",
      "epoch: 35 loss: 1.033104868934988\n",
      "epoch: 36 loss: 1.0270116458322243\n",
      "epoch: 37 loss: 1.0275758949096145\n",
      "epoch: 38 loss: 1.0183830833375684\n",
      "epoch: 39 loss: 1.0263676010644331\n",
      "epoch: 40 loss: 1.015695874312549\n",
      "epoch: 41 loss: 1.0202935405843303\n",
      "epoch: 42 loss: 1.021570844328697\n",
      "epoch: 43 loss: 1.0154252812197249\n",
      "epoch: 44 loss: 1.0150248737650276\n",
      "epoch: 45 loss: 1.0088387774577894\n",
      "epoch: 46 loss: 1.0043633874392979\n",
      "epoch: 47 loss: 1.0082556340503699\n",
      "epoch: 48 loss: 1.006462885816622\n",
      "epoch: 49 loss: 1.001739097533019\n",
      "epoch: 50 loss: 0.9982256559610508\n",
      "epoch: 51 loss: 1.0068443566241674\n",
      "epoch: 52 loss: 1.0075707758562722\n",
      "epoch: 53 loss: 0.9981800937855585\n",
      "epoch: 54 loss: 0.99977543213863\n",
      "epoch: 55 loss: 0.9984161611240258\n",
      "epoch: 56 loss: 0.999487498584888\n",
      "epoch: 57 loss: 0.9923594080417022\n",
      "epoch: 58 loss: 0.9853434203282462\n",
      "epoch: 59 loss: 0.9782561348548025\n",
      "epoch: 60 loss: 0.9662293412715471\n",
      "epoch: 61 loss: 0.9654484856609525\n",
      "epoch: 62 loss: 0.9711294252029338\n",
      "epoch: 63 loss: 0.9649457043193944\n",
      "epoch: 64 loss: 0.9635558269771322\n",
      "epoch: 65 loss: 0.9668809653992585\n",
      "epoch: 66 loss: 0.9662122276523731\n",
      "epoch: 67 loss: 0.9668752073795871\n",
      "epoch: 68 loss: 0.9610940331477971\n",
      "epoch: 69 loss: 0.9543134676320738\n",
      "epoch: 70 loss: 0.9545051824416265\n",
      "epoch: 71 loss: 0.952762552885623\n",
      "epoch: 72 loss: 0.9455827861951984\n",
      "epoch: 73 loss: 0.9491445833563571\n",
      "epoch: 74 loss: 0.9455538075942018\n",
      "epoch: 75 loss: 0.9422935383178184\n",
      "epoch: 76 loss: 0.9407385203734716\n",
      "epoch: 77 loss: 0.942860099473948\n",
      "epoch: 78 loss: 0.9378429939189836\n",
      "epoch: 79 loss: 0.9380926183712055\n",
      "epoch: 80 loss: 0.936784792399848\n",
      "epoch: 81 loss: 0.9338861539691932\n",
      "epoch: 82 loss: 0.9324279398961001\n",
      "epoch: 83 loss: 0.9321666817764124\n",
      "epoch: 84 loss: 0.92837137294015\n",
      "epoch: 85 loss: 0.9295037615898487\n",
      "epoch: 86 loss: 0.9264763801826754\n",
      "epoch: 87 loss: 0.9265783867111038\n",
      "epoch: 88 loss: 0.9248188230308665\n",
      "epoch: 89 loss: 0.9353823212155759\n",
      "epoch: 90 loss: 0.9357910679790339\n",
      "epoch: 91 loss: 0.9352963194143515\n",
      "epoch: 92 loss: 0.9269399190665479\n",
      "epoch: 93 loss: 0.9306556335766345\n",
      "epoch: 94 loss: 0.9253514125124518\n",
      "epoch: 95 loss: 0.9275218643145138\n",
      "epoch: 96 loss: 0.924148908355704\n",
      "epoch: 97 loss: 0.9241701571728747\n",
      "epoch: 98 loss: 0.9223738465843793\n",
      "epoch: 99 loss: 0.9208527190704234\n",
      "epoch: 100 loss: 0.9188918136858815\n",
      "epoch: 101 loss: 0.918819395604373\n",
      "epoch: 102 loss: 0.9173638410941213\n",
      "epoch: 103 loss: 0.9189284226729484\n",
      "epoch: 104 loss: 0.9170064565944399\n",
      "epoch: 105 loss: 0.9177011594622321\n",
      "epoch: 106 loss: 0.9268697920910755\n",
      "epoch: 107 loss: 0.9322042023755158\n",
      "epoch: 108 loss: 0.9277330780583922\n",
      "epoch: 109 loss: 0.9262771163582085\n",
      "epoch: 110 loss: 0.9229445173319452\n",
      "epoch: 111 loss: 0.9230788581089223\n",
      "epoch: 112 loss: 0.9216586738259803\n",
      "epoch: 113 loss: 0.9199128333012082\n",
      "epoch: 114 loss: 0.9204247506873421\n",
      "epoch: 115 loss: 0.9195456964295328\n",
      "epoch: 116 loss: 0.918166237344334\n",
      "epoch: 117 loss: 0.9167959034051845\n",
      "epoch: 118 loss: 0.9155710367346611\n",
      "epoch: 119 loss: 0.9143538722012227\n",
      "epoch: 120 loss: 0.9134234153388099\n",
      "epoch: 121 loss: 0.9143852111101315\n",
      "epoch: 122 loss: 0.9115929220604941\n",
      "epoch: 123 loss: 0.9120110956516985\n",
      "epoch: 124 loss: 0.9109268923251325\n",
      "epoch: 125 loss: 0.9100163023294946\n",
      "epoch: 126 loss: 0.908599385627361\n",
      "epoch: 127 loss: 0.9088980361512641\n",
      "epoch: 128 loss: 0.9079427045016111\n",
      "epoch: 129 loss: 0.9078391134304185\n",
      "epoch: 130 loss: 0.905732872108044\n",
      "epoch: 131 loss: 0.9057311646979447\n",
      "epoch: 132 loss: 0.9047368829508016\n",
      "epoch: 133 loss: 0.9053696937702956\n",
      "epoch: 134 loss: 0.9039880979106402\n",
      "epoch: 135 loss: 0.9036639941795604\n",
      "epoch: 136 loss: 0.9025785733492524\n",
      "epoch: 137 loss: 0.9024592373792714\n",
      "epoch: 138 loss: 0.9013927100420385\n",
      "epoch: 139 loss: 0.9018863244818462\n",
      "epoch: 140 loss: 0.900953376607636\n",
      "epoch: 141 loss: 0.9003582769089723\n",
      "epoch: 142 loss: 0.8998035002565187\n",
      "epoch: 143 loss: 0.899783385411749\n",
      "epoch: 144 loss: 0.898095000045584\n",
      "epoch: 145 loss: 0.8982688499474576\n",
      "epoch: 146 loss: 0.8977101220459265\n",
      "epoch: 147 loss: 0.8973168875180033\n",
      "epoch: 148 loss: 0.8958834093792203\n",
      "epoch: 149 loss: 0.8968835390868563\n",
      "epoch: 150 loss: 0.8958095493904925\n",
      "epoch: 151 loss: 0.8945799990294374\n",
      "epoch: 152 loss: 0.8949752273674986\n",
      "epoch: 153 loss: 0.8956260707316325\n",
      "epoch: 154 loss: 0.8940845640096375\n",
      "epoch: 155 loss: 0.8943166093698768\n",
      "epoch: 156 loss: 0.892961463248736\n",
      "epoch: 157 loss: 0.8926746751492741\n",
      "epoch: 158 loss: 0.8917702480874705\n",
      "epoch: 159 loss: 0.8917775631819223\n",
      "epoch: 160 loss: 0.8911562513640208\n",
      "epoch: 161 loss: 0.8914519492108939\n",
      "epoch: 162 loss: 0.8902435113640578\n",
      "epoch: 163 loss: 0.8901270728145017\n",
      "epoch: 164 loss: 0.8896709464007565\n",
      "epoch: 165 loss: 0.8891584463373133\n",
      "epoch: 166 loss: 0.8898536845453064\n",
      "epoch: 167 loss: 0.8886805051863579\n",
      "epoch: 168 loss: 0.8892193912225751\n",
      "epoch: 169 loss: 0.8871750605448847\n",
      "epoch: 170 loss: 0.8876597838942014\n",
      "epoch: 171 loss: 0.8869055296819588\n",
      "epoch: 172 loss: 0.8870817498202208\n",
      "epoch: 173 loss: 0.8866650164859623\n",
      "epoch: 174 loss: 0.886709733057617\n",
      "epoch: 175 loss: 0.8855858751121186\n",
      "epoch: 176 loss: 0.8857884518481112\n",
      "epoch: 177 loss: 0.8853467805674637\n",
      "epoch: 178 loss: 0.8850991233167533\n",
      "epoch: 179 loss: 0.8843787143491275\n",
      "epoch: 180 loss: 0.8855502589397666\n",
      "epoch: 181 loss: 0.8850165334051491\n",
      "epoch: 182 loss: 0.8845371264092454\n",
      "epoch: 183 loss: 0.8832300634577911\n",
      "epoch: 184 loss: 0.8832173366430153\n",
      "epoch: 185 loss: 0.8827754405603006\n",
      "epoch: 186 loss: 0.8824881787638434\n",
      "epoch: 187 loss: 0.8821873085895882\n",
      "epoch: 188 loss: 0.882395322527202\n",
      "epoch: 189 loss: 0.8817631244490788\n",
      "epoch: 190 loss: 0.8818512353298482\n",
      "epoch: 191 loss: 0.881057229767763\n",
      "epoch: 192 loss: 0.881145233831511\n",
      "epoch: 193 loss: 0.8807274333647559\n",
      "epoch: 194 loss: 0.8801473699292567\n",
      "epoch: 195 loss: 0.8799026759996452\n",
      "epoch: 196 loss: 0.8795247898328609\n",
      "epoch: 197 loss: 0.8798845861779373\n",
      "epoch: 198 loss: 0.8789824676892332\n",
      "epoch: 199 loss: 0.8777394640473064\n",
      "epoch: 200 loss: 0.878504970846294\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, nb_epoch+1):\n",
    "    train_loss = 0\n",
    "    s = 0.0\n",
    "    for id_user in range(nb_users):\n",
    "        input_vec = Variable(training_set[id_user: id_user + 1])\n",
    "        target = input_vec.clone()\n",
    "        if torch.sum(target.data > 0) > 0: # Checking for users who have rated atleast one movie\n",
    "            output = model(input_vec)\n",
    "            target.require_grad = False\n",
    "            # Implement backprop\n",
    "            output[target == 0] = 0 # Set the values for the prediction to 0 for those \n",
    "                                    # movies that have not been rated. No need to updated these \n",
    "                                    # weights. Helps optimize code\n",
    "            loss = loss_function(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) # consider all \n",
    "                                            # movies that have non zero rating. \n",
    "                                            # 1e-10 added to make sure denom is never zero\n",
    "                                            # Mean_corrector represents avg of error that have\n",
    "                                            # non-zero rating\n",
    "            loss.backward() # BACKPROP!!!!!!!!. Decides the director to which the weights will be upadted\n",
    "            train_loss += loss.data[0]*mean_corrector\n",
    "            s += 1.0\n",
    "            optimizer.step() # Decides the amount by which the weights will be updated\n",
    "    print('epoch: '+str(i)+' loss: '+str(train_loss/s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.1623  3.7004  3.5885  ...   2.2359  3.7317  3.2989\n",
      "[torch.FloatTensor of size 1x1682]\n",
      "\n",
      "test loss: 0.953415874701\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "s = 0.\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user])\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = model(input)\n",
    "        if id_user == 0:\n",
    "            print(output)\n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0\n",
    "        loss = loss_function(output, target)\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
    "        s += 1.\n",
    "print('test loss: '+str(test_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
